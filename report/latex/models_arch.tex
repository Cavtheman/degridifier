\subsection{Model selection}
Since this project works entirely on images, variations on convolutional neural networks make for obvious choices of models. The first idea for this project was to use a U-net model with paired grid and nogrid images for supervised learning, though these proved difficult to find. After trying the U-net model on artificial data, I realised that the model really only needed to actually learn very little (i.e. where the grid lines are), I experimented with adding a residual connection to it. This performed significantly better, and thus I implemented a full ResUnet after. Wanting to see whether the performance of this could be improved still, I turned to a cycleGAN architecture, using the ResUnet as a base for the generators. All models have been implemented using PyTorch, and all the code can be found \href{https://github.com/Cavtheman/degridifier}{here.} All models are also optimised using the ADAM optimiser.
%The intention for this project was to only implement a U-net model and a CycleGAN, but as the project progressed and the U-net alone did not perform as hoped, I also implemented a ResUnet model.
\subsection{U-net}
The U-net architecture is well suited for this kind of task consisting of images of arbitrary size. Because a U-net is fully convolutional and thus does not have any fully connected layers, it has very little trouble with images of different size than the training data. However, because the field of view is fixed it is not size invariant, and as such, a CNN may have trouble when looking at images and features with large ranges of resolution, as is the case in this dataset. The U-net architecture, usually used for classification, works by gradually encoding information about \textit{what} is in the image on the way down, and then augmenting this data with more positional information on the way up. That is, on the way up it decodes the \textit{what} and tries to give it a \textit{where}. That analogy still holds for this kind of problem, but in this case it also has to learn how to reconstruct the original image as well as finding and removing the grids.

The U-net model has been implemented as per \cite{unet}, with a few exceptions. Since the output image should have three colour channels, the final convolution outputs just that, instead of the one channel shown in Figure \ref{img:architectures}. The model also does not need to crop before concatenating, since I have instead chosen to pad the images, using the ``reflect'' padding mode from PyTorch. This padding method was chosen because it would still keep the grid lines in the padded pixels. Additionally, all uses of the ReLU activation function have been replaced with ELU activation functions. This was done to prevent potential dead neurons from appearing, by giving a small gradient for values < 0. The model is also not as deep. By this I mean that when encoding, the amount of channels used does not go up to 896, instead it goes from 8 in the top layer, to 16, 32 and then 64 in the bottommost layer. While evaluating the performance of this kind of model can be done with quantitative methods, a good score on those do not always translate to good results to the human eyes, as can be seen in examples further down. I use the mean absolute error (L1) loss function and mean squared error (L2) for training and quantitative evaluation, as implemented in PyTorch, and seen in Equation \ref{eq:l1_loss}. The mean squared error is also experimented with, and can be seen in Equation \ref{eq:l2_loss}.
\begin{align*}
  &\text{For input $x\in X$ and target $y\in Y$ where $x$ and $y$ are tensors of any arbitrary shape, with $N$ elements.}
\end{align*}
\begin{align}
  \mathcal{L}_{L1}(X,Y) &= \frac{1}{N} \sum^N_{i=1} |x_i - y_i| \label{eq:l1_loss}\\
  \mathcal{L}_{L2}(X,Y) &= \frac{1}{N} \sum^N_{i=1} (x_i - y_i)^2 \label{eq:l2_loss}
\end{align}

\begin{figure}[H]
  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.25\linewidth]{images/resunet_architecture.png}
  \includegraphics[width=0.6\linewidth]{images/unet_architecture.png}
  \caption[center]{Left: ResUnet architecture from \cite{resunet}. Right: U-net architecture from \cite{unet}.}\label{img:architectures}
\end{figure}

\subsection{ResUnet}
The ResUnet is intended as an improvement to the U-net, and is likely more useful in this task than many other similar tasks. The reason for this is that the ResUnet removes the need for the model to learn the reconstruction of the image on top of changing it. Instead, the model will now simply have to learn where the output is \textit{different} from the input, and by how much. However, the shape of the data running through the net means that without some form of projection or mapping to different dimensions. I have made these as projection shortcuts as described in \cite{residual}. They find that projection shortcuts, as opposed to zero-padding, perform marginally better, though they add memory and time complexity.

The ResUnet has been implemented as described in \cite{resunet}, with the addition of an additional residual connection directly between the input and output. This intention behind this addition is much the same as for the other residual blocks. The difference here is then that no projection is required, and means that the model truly does not need to learn how to reconstruct the image, as the output of the model before this is added onto the image, pixelwise. I have also chosen to use ELU instead of ReLU here for any activation functions in the original, for the same reason as the U-net. The depth of the ResUnet models trained are identical to the regular U-net models, using 6, 12, 24 and 48 channels respectively going down the encoding due to slightly higher memory footprint. One other change from the U-net that may be significant is the fact that there is no dropout in the bridge part of the model. The reasoning for this is that no such thing is mentioned in the paper whose architecture I implemented, though adding it would likely make the model more robust. Like the U-net, training and quantitative evaluation is done using L1 loss.

\subsection{CycleGAN}
The cycleGAN architecture has been implemented as described in \cite{cyclegan}. The idea behind choosing to use this kind of model is that the leftover artifacts of the previous models were still in a regular grid, which should be quite easy for a discriminator to detect. The output of this model is then used to help the generator model get rid of any traces of the grid.

The base generators of the model are still the ResUnet. For easier training, the cycleGAN ``nogrid'' generator that removes grids from images starts off with a pretrained version of the aforementioned ResUnet, trained on artificial data. This pretraining should help the model start learning the important features more quickly, instead of spending time learning the basics.

Training this kind of model is significantly more finicky than the other two. This is in part due to the use and combination of several different kinds of loss functions; balancing these in a good way seems slightly arbitrary, and experimentation is required to find good values. Due to the cyclical nature of the training, the models can also ``go off on a tangent'', outputting more and more absurd things, becoming less and less useful. When this happens the model will often not come back to outputting anything useful even after more training. The loss functions used for this model consist of several different parts. Another reason for the difficulty of training this kind of model is that there is no supervision to make sure that the model learns reasonable things. This means that it is easy for it to diverge from outputting anything useful.

\subsubsection{Identity loss}
The first loss value to look at is the identity loss. The idea behind this is that a generator $G_{X\rightarrow Y}$, given $Y$ should not change the image at all, since it is already of the same category. Thus, during training each generator is given an image of the category it is supposed to convert to, and L1 loss is again used to determine performance. This loss function is evaluated on both the grid and nogrid generators. I will talk about the lambda value in the experiments section.
\begin{align}
  &\text{For generator $G_{X\rightarrow Y}$ and image $Y$:} \nonumber\\
  &\mathcal{L}_{Id}(G,Y) = \lambda_{Id} \mathcal{L}_{L1}\left(G(Y),Y\right)\label{eq:idlambda}
\end{align}

\subsubsection{GAN loss}
This loss is slightly more complicated than the others, but is just as important. The idea behind GAN loss, or adversarial loss, is that a generator tries to ``trick'' a discriminator into thinking that its' output is a real example of the data it is trying to generate. One important thing to note is that given a large enough model, you can teach it to train it to output any other permutation of images, given the same data. Thus, there is also need for cycle loss. The formulation is as follows:
\begin{align}
  &\text{For generator $G_{X\rightarrow Y}$, discriminator $D_Y$, input $x \in X$ and target $y \in Y$:} \nonumber\\
  &\mathcal{L}_{GAN}(G_{X\rightarrow Y},D_Y,X,Y) = -\frac{\lambda_{GAN}}{N} \sum_{i=1}^N log(D_Y(y)) + log(1-D_Y(G(x)))
  %y_i \cdot log\left(D_Y(G(x_i))\right) + (1-y_i) \cdot log\left(1-D_Y(G(x_i))\right)\label{eq:ganlambda}
\end{align}
With this, it then tries to find $min_Gmax_{D_Y}\left(\mathcal{L}_{GAN}(G_{X\rightarrow Y},D_Y,X,Y)\right)$. This loss is applied both ways for both $G_{X\rightarrow Y}, D_Y$ and $G_{Y\rightarrow X}, D_X$.

\subsubsection{Cycle loss}
The idea behind cycle loss is that if both of the generators are good, then converting from $X$ to $Y$, then back to $X$ should look identical to $X$. That is:
\begin{align*}
  X \approx G_{Y\rightarrow X}(G_{X\rightarrow Y}(X))
\end{align*}
Thus, I define a cycle loss as follows:
\begin{align}
  \mathcal{L}_{cyc}(G_{Y\rightarrow X}, G_{X\rightarrow Y}, X, Y) =& \lambda_{cyc} \bigg(\mathcal{L}_{L1}\big( G_{Y\rightarrow X}(G_{X\rightarrow Y}(X)), X\big)\nonumber\\
  &+ \mathcal{L}_{L1}\big( G_{X\rightarrow Y}(G_{Y\rightarrow X}(Y)), Y\big)\bigg)\label{eq:cyclambda}
\end{align}
This type of evaluation is also used in other contexts and problems, including translation. In that case, it makes sense that if you can translate a sentence to another language, and then back without losing the meaning, then at the very least you have some consistency in the translations.
