\subsection{U-net}
None of the U-net variants performed nearly as well as I hoped, unfortunately. The sigmoid variants especially do not perform well, though this may well be due to an implementation error. For quantitative evaluation, L1 loss is performed, though this will only tell how good an image is pixel-for-pixel, not how well the model actually does on the grids. A model could output random noise and have removed the grid entirely, but still have a bad score, for example. One could train a separate discriminator and use that to evaluate ``goodness'' more quantitatively, though I do not have the time to do this. Thus, the qualitative evaluation of the images must suffice. While none of the models overfit on the training data compared to the validation data, this is to be expected due to the data generation method. During training, each image is augmented randomly, and a random grid is added. This is done differently each epoch, so the model has no way of learning the specific grids in the training data, and thus has no way to overfit on it compared to the validation data. The validation data is essentially no different to the model than the training data.

\subsubsection{Final layer variants}
This experiment shows that there is only very minimal difference between the versions with and without the tanh activation function, which can probably simply be attributed to variance in training. When also looking at the variant with the sigmoid activation there is a significant difference however. When looking solely at the loss values it does significantly worse than the other two, and when you take a look at the images it generates, that makes sense. The model seems to have trouble generating the full range of colour and essentially seems to apply a sepia filter. This may be because of the sigmoid functions tendency to avoid values close to $0$ or $1$, but there is likely an implementation error somewhere that exacerbates this. It is an interesting note however, that the sigmoid variant seems to be better at getting rid of the gird lines than the other two variants. This is especially noticeable in the darker parts of the images where the grids are much less visible.

\subsubsection{L1 vs L2 loss}
There is actually a relatively significant difference between the outputs of models trained using L1 loss compared to L2. While each of the L2 models score worse on the actual loss values, the final output seems to have dimmed the grids a good bit more, though they are still visible. The slight blurring that is typically associated with using L2 loss on images like this may actually be causing this slight dimming in the residual variants. Each of the residual examples' outputs have actually turned the grids slightly pink, and I suspect that training them for longer would make them a bit better, considering that the loss graphs still seem to be going down ever so slightly. The results on the sigmoid variant are not as noticeable as for the other two, but the grids do appear slightly dimmer, though this may be a side effect of the entire image being dimmer.

\subsubsection{Performance on real data}
The model used for this experiment was the ``tanh activaition + residual'' model trained with L2 loss. Since this model was trained exclusively on artificial data it does not come as a surprise that it does not perform well on real data. One thing that is noticeable in these images more than the previous examples is that there is a slight discolouration.

\subsection{ResUnet}
Each of the ResUnet variants performed significantly better visually, compared to their U-net counterparts, despite their almost across the board worse L1 loss scores on the test set. The sigmoid variants definitely have a large improvement compared to the U-net variant, though there are still issues with them. It seems that there is still room to improve for several of these models actually, when looking at the loss graphs in Figure \ref{image:resunet_loss}. Several of them seem to still have a slight downward slope, though it should be noted that the y-axis is logarithmic.

\subsubsection{Final layer variants}
In the ResUnet there is a slightly larger difference between the variants with residual last layers than in the U-net, though that may still come down to variance in training. What is interesting in this experiment is that the sigmoid variant seems to be significantly better at removing the grids than the other two variants. This comes at the expense of the variant being significantly worse at reconstructing the rest of the image however. This is especially noticeable in the darker parts of the left image, where the skeletons have been made significantly darker and more difficult to see. There also seems to be some slight discolouration in all of the images, though it is most noticeable in the sigmoid variant. An interesting observation of the loss graph (Figure \ref{image:resunet_loss}) is that the sigmoid variant's validation loss during training is a good deal lower than one would expect given the training loss, especially the one trained with L1 loss.

\subsubsection{L1 vs L2 loss}
In this case again there seems to be a very significant difference between the models trained using L1 vs L2 loss. It seems that my hypothesis that using L1 loss would not ``punish'' errors on the grids enough is true in this case. However, there seems to be discolouration of the image in places where they are dark in all the L2 models, which is undesirable. However, it may be that some more training time would reduce this colouration, since it seems to be primarily be in the red and blue channels. The sigmoid variant changes the image quite significantly on top of the discolouration, and though the L1 variant seems to be the best so far at removing the grid, the changes make it undesirable.

\subsubsection{Dropout}
Contrary to what is usually experienced with dropout during training, it seems that adding dropout to this network does not significantly help with performance or robustness compared to a model trained without it. Visually the models actually seem to perform slightly worse. I would hypothesise that this is because of the residual layers partially removing the need for learning redundancies. Though I have not had the chance to test this, I would imagine that the sigmoid variant would, in fact, perform better with some dropout applied.

\subsubsection{Performance on real data}
Due to the large difference in performance between the residual and sigmoid variants, I have chosen to use both the ``residual + no activation function'' trained using L2 loss and and the sigmoid variant trained using L1 loss. We see here that the former of the two variants manages to blur some of the grids very slightly, but otherwise does not change the images too much. The sigmoid variant however removes large splotches of the images. It seems to do this mostly where the original image is bright. Though it is not visible in the examples given above, it also happens in the artificial data.

\subsection{CycleGAN}
In the end I could not figure out how to make any of the cycleGAN variants work, though I can hypothesise why they do not work. None of the outputs from the models seem to be able to reconstruct the base images, even with grids, despite the fact that they were all based on a pretrained ResUnet network which was able to perform decently. I have a few hypotheses as to why this might be the case.

The first explanation that comes to mind, comes from the very abnormal loss curves, seen in Figure \ref{image:cyclegan_loss}. The loss values for both the discriminators and the generators seem to go up and down in what looks almost like sine waves, though these waves are more pronounced in some variants than ohters. One interesting thing to note regarding these waves is that the peaks and troughs in the discriminator are inversely correlated with the peaks and troughs of the generator. This leads me to believe that the problem lies with the discriminator. Once the generator becomes good enough, the discriminator quickly becomes unable to distinguish between the two categories. This means that the backpropagation begins to ``teach'' the generator things that are not beneficial.

Another potential explanation can be gleaned from the images in Figure \ref{image:cyclegan_images}. It seems that the grid and nogrid generators very quickly devolve into essentially colouring their images opposite colours, so that the cycle loss is minimal. One possible way to help the grid generator learn is to give it a pretrained nogrid ResUnet, as the tasks are similar. The idea being that the model has already learned the structure of a grid, and then only needs to learn how to add one.

I should note that I realised shortly before the deadline that I was using an incorrect configuration of the ResUnet, but did not have time to perform the entire range of experiments again with the correct one. The ones I did perform still had the abnormal loss curves and discolouration, but the nogrid generator was able to remove grids on black backgrounds partially, but nowhere else. The grid generator still did not add any kind of grid. Loss curves and examples can be seen in Figures \ref{image:cyclegan_fixed_loss} and \ref{image:cyclegan_fixed_images}. Interestingly, the L1 loss values are not much different than the ``incorrect'' variants, but the final result looks significantly better, comparatively. The results are still significantly worse than those from the ResUnet.
