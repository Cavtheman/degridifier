{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# My own files\n",
    "from data_generator import CombinedDataset\n",
    "from discriminator import Discriminator\n",
    "from resunet import ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "\n",
    "# Initialising the data generator and model\n",
    "grid_path = \"data/train/grid/\"\n",
    "nogrid_path = \"data/train/nogrid/\"\n",
    "\n",
    "batch_size = 1\n",
    "max_imgs = 5000\n",
    "\n",
    "augments = {\"crop\" : (400,400),\n",
    "            \"hflip\" : 0.5,\n",
    "            \"vflip\" : 0.5,\n",
    "            \"angle\" : 0,\n",
    "            \"shear\" : 0,\n",
    "            \"brightness\" : (0.75,1.25),\n",
    "            \"pad\" : (0,0,0,0),\n",
    "            \"contrast\" : (0.5,2.0)}\n",
    "\n",
    "dataset = CombinedDataset(max_imgs, grid_path, nogrid_path, **augments)\n",
    "validation_set = CombinedDataset(max_imgs, grid_path, nogrid_path, **augments, seed=1337)\n",
    "\n",
    "training_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "validation_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "#loaded_params = torch.load(\"lstm_1_layer_with_schedule_1024_minloss.pth\")\n",
    "#model = LSTM_model(**loaded_params[\"args_dict\"]).to(processor)\n",
    "#model.load_state_dict(loaded_params[\"state_dict\"])\n",
    "\n",
    "nogrid_disc = Discriminator (3).to(processor)\n",
    "grid_disc = Discriminator (3).to(processor)\n",
    "\n",
    "\n",
    "loaded_params = torch.load(\"res_200_epoch_early.pth\")\n",
    "nogrid_gen = ResUnet (**loaded_params[\"args_dict\"]).to(processor)\n",
    "nogrid_gen.load_state_dict(loaded_params[\"state_dict\"])\n",
    "grid_gen = ResUnet (3, 3).to(processor)\n",
    "\n",
    "cycle_loss = nn.L1Loss().to(processor)\n",
    "id_loss = nn.L1Loss().to(processor)\n",
    "# Why not BCE?\n",
    "adv_loss = nn.MSELoss().to(processor)\n",
    "\n",
    "gen_opt = optim.Adam (chain (nogrid_gen.parameters(), grid_gen.parameters()), lr=1e-3)\n",
    "nogrid_disc_opt = optim.Adam (nogrid_disc.parameters(), lr=1e-3)\n",
    "grid_disc_opt = optim.Adam (grid_disc.parameters(), lr=1e-3)\n",
    "                              \n",
    "#scheduler = optim.lr_scheduler.StepLR (optimiser, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialising some variables for use in training\n",
    "batches = float(\"inf\")\n",
    "time_diff = 0\n",
    "no_improv = 0\n",
    "min_loss = float(\"inf\")\n",
    "val_loss = float(\"inf\")\n",
    "min_val_loss = float(\"inf\")\n",
    "epochs = 50\n",
    "early_stop_epoch = 0\n",
    "\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "\n",
    "id_losses = []\n",
    "gan_losses = []\n",
    "cycle_losses = []\n",
    "\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(training_generator):\n",
    "\n",
    "        # Keeping track of stuff\n",
    "        start_time = datetime.now()\n",
    "        est_time_left = str(time_diff*(min(batches, dataset.__len__()) - i)+time_diff*(epochs-(epoch+1))*min(batches, dataset.__len__()/batch_size)).split(\".\")[0]\n",
    "        \n",
    "        sys.stdout.write(\"\\rEpoch: {0}. Batch: {1}. Min loss: {2:.5f}. Time left: {3}. Best: {4} batches ago. Val loss: {5:.5f}\".format(epoch+1, i+1, min_loss, est_time_left, no_improv, val_loss))\n",
    "\n",
    "        # Putting data on gpu\n",
    "        real_grid = batch[\"grid\"].to(processor)\n",
    "        real_nogrid = batch[\"nogrid\"].to(processor)\n",
    "\n",
    "        real_label = torch.full((batch_size, 1), 1, device=processor, dtype=torch.float32)\n",
    "        fake_label = torch.full((batch_size, 1), 0, device=processor, dtype=torch.float32)\n",
    "        \n",
    "        nogrid_disc.train()\n",
    "        nogrid_gen.train()\n",
    "        grid_disc.train()\n",
    "        grid_gen.train()\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        # Training the generators\n",
    "\n",
    "        # Identity loss\n",
    "        id_grid = grid_gen (real_grid)\n",
    "        id_grid_loss = id_loss (id_grid, real_grid)\n",
    "\n",
    "        id_nogrid = grid_gen (real_nogrid)\n",
    "        id_nogrid_loss = id_loss (id_nogrid, real_nogrid)\n",
    "\n",
    "        \n",
    "        # GAN loss\n",
    "        fake_grid = grid_gen (real_nogrid)\n",
    "        fake_grid_pred = grid_disc (fake_grid)\n",
    "        gan_loss_grid = adv_loss (fake_grid_pred, real_label)\n",
    "\n",
    "        fake_nogrid = nogrid_gen(real_grid)\n",
    "        fake_nogrid_pred = nogrid_disc (fake_nogrid)\n",
    "        gan_loss_nogrid = adv_loss (fake_nogrid_pred, real_label)\n",
    "\n",
    "        \n",
    "        # Cycle loss\n",
    "        cycled_grid = grid_gen (fake_nogrid)\n",
    "        cycle_loss_grid = cycle_loss (cycled_grid, real_grid)\n",
    "\n",
    "        cycled_nogrid = nogrid_gen (fake_grid)\n",
    "        cycle_loss_nogrid = cycle_loss (cycled_nogrid, real_nogrid)\n",
    "        \n",
    "        gen_loss = id_grid_loss + id_nogrid_loss + gan_loss_grid + gan_loss_nogrid + cycle_loss_grid + cycle_loss_nogrid\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Training the discriminators\n",
    "\n",
    "        # Grid discriminator\n",
    "        grid_disc_opt.zero_grad()\n",
    "\n",
    "        real_grid_disc = grid_disc (real_grid)\n",
    "        real_grid_disc_loss = adv_loss (real_grid_disc, real_label)\n",
    "\n",
    "        fake_grid_disc = grid_disc (fake_grid.detach())\n",
    "        fake_grid_disc_loss = adv_loss (fake_grid_disc, fake_label)\n",
    "\n",
    "        grid_disc_loss = (real_grid_disc_loss + fake_grid_disc_loss)/2 # Is /2 necessary?\n",
    "        \n",
    "        grid_disc_loss.backward()\n",
    "        grid_disc_opt.step()\n",
    "        \n",
    "\n",
    "        # Nogrid discriminator\n",
    "        grid_disc_opt.zero_grad()\n",
    "\n",
    "        real_nogrid_disc = nogrid_disc (real_nogrid)\n",
    "        real_nogrid_disc_loss = adv_loss (real_nogrid_disc, real_label)\n",
    "\n",
    "        fake_nogrid_disc = nogrid_disc (fake_nogrid.detach())\n",
    "        fake_nogrid_disc_loss = adv_loss (fake_nogrid_disc, fake_label)\n",
    "\n",
    "        nogrid_disc_loss = (real_nogrid_disc_loss + fake_nogrid_disc_loss)/2 # Is /2 necessary?\n",
    "\n",
    "        nogrid_disc_loss.backward()\n",
    "        nogrid_disc_opt.step()\n",
    "\n",
    "        gen_losses.append(gen_loss.item())\n",
    "        disc_losses.append((grid_disc_loss + nogrid_disc_loss).item())\n",
    "        \n",
    "        id_losses.append((id_grid_loss + id_nogrid_loss).item())\n",
    "        gan_losses.append((gan_loss_grid + gan_loss_nogrid).item())\n",
    "        cycle_losses.append((cycle_loss_grid + cycle_loss_nogrid).item())\n",
    "        \n",
    "        \n",
    "        if gen_loss.item() < min_loss:\n",
    "            #model.save(\"temp_best_model.pth\")\n",
    "            #torch.save(model.state_dict(), \"temp_best_model.pth\")\n",
    "            min_loss = gen_loss.item()\n",
    "            no_improv = 0\n",
    "        else:\n",
    "            no_improv += 1\n",
    "    \n",
    "        # For tracking progress\n",
    "        end_time = datetime.now()\n",
    "        time_diff = end_time - start_time\n",
    "\n",
    "    val_loss = 0\n",
    "    nogrid_disc.eval()\n",
    "    nogrid_gen.eval()\n",
    "    grid_disc.eval()\n",
    "    grid_gen.eval()\n",
    "    for batch in validation_generator:\n",
    "        # Putting data on gpu\n",
    "        real_grid = batch[\"grid\"].to(processor)\n",
    "        real_nogrid = batch[\"nogrid\"].to(processor)\n",
    "        \n",
    "        real_label = torch.full((batch_size, 1), 1, device=processor, dtype=torch.float32)\n",
    "        fake_label = torch.full((batch_size, 1), 0, device=processor, dtype=torch.float32)\n",
    "        \n",
    "        # Identity loss\n",
    "        id_grid = grid_gen (real_grid)\n",
    "        id_grid_loss = id_loss (id_grid, real_grid)\n",
    "\n",
    "        id_nogrid = grid_gen (real_nogrid)\n",
    "        id_nogrid_loss = id_loss (id_nogrid, real_nogrid)\n",
    "\n",
    "        \n",
    "        # GAN loss\n",
    "        fake_grid = grid_gen (real_nogrid)\n",
    "        fake_grid_pred = grid_disc (fake_grid)\n",
    "        gan_loss_grid = adv_loss (fake_grid_pred, real_label)\n",
    "\n",
    "        fake_nogrid = nogrid_gen(real_grid)\n",
    "        fake_nogrid_pred = nogrid_disc (fake_nogrid)\n",
    "        gan_loss_nogrid = adv_loss (fake_nogrid_pred, real_label)\n",
    "\n",
    "        \n",
    "        # Cycle loss\n",
    "        cycled_grid = grid_gen (fake_nogrid)\n",
    "        cycle_loss_grid = cycle_loss (cycled_grid, real_grid)\n",
    "\n",
    "        cycled_nogrid = nogrid_gen (fake_grid)\n",
    "        cycle_loss_nogrid = cycle_loss (cycled_nogrid, real_nogrid)\n",
    "\n",
    "        gen_loss = id_grid_loss + id_nogrid_loss + gan_loss_grid + gan_loss_nogrid + cycle_loss_grid + cycle_loss_nogrid\n",
    "\n",
    "        val_loss += gen_loss.item()\n",
    "        \n",
    "    val_loss = val_loss/len(validation_generator)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_epoch = epoch\n",
    "        grid_gen.save(\"early_stop_grid_gen.pth\")\n",
    "        nogrid_gen.save(\"early_stop_nogrid_gen.pth\")\n",
    "        grid_disc.save(\"early_stop_grid_disc.pth\")\n",
    "        nogrid_disc.save(\"early_stop_nogrid_disc.pth\")\n",
    "        \n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "grid_gen.save(\"final_grid_gen.pth\")\n",
    "nogrid_gen.save(\"final_nogrid_gen.pth\")\n",
    "grid_disc.save(\"final_grid_disc.pth\")\n",
    "nogrid_disc.save(\"final_nogrid_disc.pth\")\n",
    "print (\"\\nFinished. Early stop was at:\", early_stop_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the loss through the epochs\n",
    "train_placings = np.linspace(0,epoch+1,len(gen_losses))\n",
    "val_placings = np.arange (1, epoch+2)\n",
    "\n",
    "plt.plot(train_placings, gen_losses, label=\"Training\")\n",
    "plt.plot(val_placings, val_loss_list, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Generator Loss during training\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"gan_loss_log.png\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "cycle_gan.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
