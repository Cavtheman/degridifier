{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# My own files\n",
    "from gridifier import gridify\n",
    "from data_generator import ArtificialDataset\n",
    "from unet import Unet\n",
    "from resunet import ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "\n",
    "# Initialising the data generator and model\n",
    "labels_path = \"data/train/nogrid/\"\n",
    "val_path = \"data/val/nogrid/\" \n",
    "\n",
    "batch_size = 4\n",
    "max_imgs = 10000\n",
    "\n",
    "augments = {\"grid_size\" : (30,90),\n",
    "            \"grid_intensity\" : (-1,1),\n",
    "            \"grid_offset_x\" : (0,90),\n",
    "            \"grid_offset_y\" : (0,90),\n",
    "            \"crop\" : (400,400),\n",
    "            \"hflip\" : 0.5,\n",
    "            \"vflip\" : 0.5,\n",
    "            \"angle\" : 0,\n",
    "            \"shear\" : 0,\n",
    "            \"brightness\" : (0.75,1.25),\n",
    "            \"pad\" : (0,0,0,0),\n",
    "            \"contrast\" : (0.5,1.25)}\n",
    "\n",
    "dataset = ArtificialDataset(max_imgs, labels_path, **augments)\n",
    "validation_set = ArtificialDataset(max_imgs, val_path, **augments, seed=1337)\n",
    "\n",
    "training_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "validation_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "\n",
    "#model = Unet (3,3).to(processor)\n",
    "model = ResUnet (3,3).to(processor)\n",
    "\n",
    "loss_function = nn.L1Loss().to(processor)\n",
    "#loss_function = nn.MSELoss().to(processor)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.5)\n",
    "#print (dataset.__get_all_files__(\"data/nogrid/\"))\n",
    "#dataset.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialising some variables for use in training\n",
    "batches = float(\"inf\")\n",
    "time_diff = 0\n",
    "no_improv = 0\n",
    "min_loss = float(\"inf\")\n",
    "val_loss = float(\"inf\")\n",
    "min_val_loss = float(\"inf\")\n",
    "epochs = 200\n",
    "early_stop_epoch = 0\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (batch, labels) in enumerate(training_generator):\n",
    "\n",
    "        # Keeping track of stuff\n",
    "        start_time = datetime.now()\n",
    "        est_time_left = str(time_diff*(min(batches, dataset.__len__()) - i)+time_diff*(epochs-(epoch+1))*min(batches, dataset.__len__()/batch_size)).split(\".\")[0]\n",
    "        \n",
    "        sys.stdout.write(\"\\rEpoch: {0}. Batch: {1}. Min loss: {2:.5f}. Time left: {3}. Best: {4} batches ago. Val loss: {5:.5f}\".format(epoch+1, i+1, min_loss, est_time_left, no_improv, val_loss))\n",
    "\n",
    "        # Putting data on gpu\n",
    "        batch = batch.to(processor)\n",
    "        labels = labels.to(processor)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "\n",
    "        out = model(batch)\n",
    "        loss = loss_function(out, labels)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        if loss.item() < min_loss:\n",
    "            #model.save(\"temp_best_model.pth\")\n",
    "            #torch.save(model.state_dict(), \"temp_best_model.pth\")\n",
    "            min_loss = loss.item()\n",
    "            no_improv = 0\n",
    "        else:\n",
    "            no_improv += 1\n",
    "        \n",
    "        # For tracking progress\n",
    "        end_time = datetime.now()\n",
    "        time_diff = end_time - start_time\n",
    "    \n",
    "        \n",
    "        #if batch.size()[0] == 1:\n",
    "        #    to_save = torch.cat ((labels, out, batch), dim=2)\n",
    "        #else:\n",
    "        #    to_save = torch.cat ((labels, out.squeeze(0), batch), dim=2)\n",
    "        #    #print(to_save.size())\n",
    "        #save_image(to_save, \"data/artificialgrid/{0}.png\".format(i))\n",
    "\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch, labels in validation_generator:\n",
    "        batch = batch.to(processor)\n",
    "        labels = labels.to(processor)\n",
    "        \n",
    "        \n",
    "        out = model(batch)\n",
    "        loss = loss_function(out, labels)\n",
    "        val_loss += loss.item()\n",
    "    val_loss = val_loss/len(validation_generator)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_epoch = epoch\n",
    "        model.save(\"early_stop_model.pth\")\n",
    "    val_loss_list.append(val_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "model.save(\"temp_model.pth\")\n",
    "print (\"\\nFinished. Early stop was at:\", early_stop_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the loss through the epochs\n",
    "train_placings = np.linspace(0,epoch+1,len(loss_list))\n",
    "val_placings = np.arange (1, epoch+2)\n",
    "\n",
    "plt.plot(train_placings, loss_list, label=\"Training\")\n",
    "plt.plot(val_placings, val_loss_list, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Loss during training\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "unet_experiment.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
