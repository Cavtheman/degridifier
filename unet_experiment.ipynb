{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Upsample\n",
    "from torch.nn import MaxPool2d\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# My own files\n",
    "from gridifier import gridify\n",
    "from data_generator import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth=[8,16,32,64], dropout=0.5):\n",
    "        super(Unet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pool = MaxPool2d((2,2))\n",
    "\n",
    "        #Convolutions on the way down\n",
    "        self.convd1_0 = Conv2d (in_channels, depth[0], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convd1_1 = Conv2d (depth[0], depth[0], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        self.convd2_0 = Conv2d (depth[0],  depth[1], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convd2_1 = Conv2d (depth[1],  depth[1], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        self.convd3_0 = Conv2d (depth[1], depth[2], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convd3_1 = Conv2d (depth[2], depth[2], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        self.convd4_0 = Conv2d (depth[2], depth[3], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convd4_1 = Conv2d (depth[3], depth[3], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        # Convolutions on the way up\n",
    "        self.convu1_0 = Conv2d (depth[3], depth[2], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convu1_1 = Conv2d (depth[2], depth[2], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        self.convu2_0 = Conv2d (depth[2], depth[1], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convu2_1 = Conv2d (depth[1], depth[1], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        self.up_conv1 = Conv2d (depth[3], depth[2], (1,1), padding=(0,0), padding_mode=\"reflect\")\n",
    "        self.up_conv2 = Conv2d (depth[2], depth[1], (1,1), padding=(0,0), padding_mode=\"reflect\")\n",
    "        self.up_conv3 = Conv2d (depth[1], depth[0], (1,1), padding=(0,0), padding_mode=\"reflect\")\n",
    "\n",
    "        # \"Horizontal\" convolutions\n",
    "        self.convu3_0 = Conv2d (depth[1], depth[0], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convu3_1 = Conv2d (depth[0], depth[0], (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "        self.convu3_2 = Conv2d (depth[0], 3, (1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        # Finding the image\n",
    "        #self.remover_conv1 = Conv2d (in_channels+1, in_channels+1, (3,3), padding=(2,2), padding_mode=\"reflect\", dilation=2)\n",
    "        self.remover_conv1 = Conv2d (in_channels+1, in_channels+1, (3,3), padding=(1,1), padding_mode=\"reflect\", dilation=1)\n",
    "        self.remover_conv2 = Conv2d (in_channels+1, out_channels, (3,3), padding=(1,1), padding_mode=\"reflect\")\n",
    "\n",
    "        \n",
    "        self.up_samp1 = Upsample (scale_factor=(2,2), mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.drop = Dropout (dropout)\n",
    "\n",
    "        #self.up_conv1 = Upsample(size=(1,depth[3],73,73), mode='nearest')\n",
    "        #self.up_conv2 = Upsample(size=(1,depth[2],146,146), mode='nearest')\n",
    "        #self.up_conv3 = Upsample(size=(1,depth[1],292,292), mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward (self, input_data):\n",
    "        # Downsampling\n",
    "        conved1_0 = F.elu (self.convd1_0 (input_data))\n",
    "        conved1_1 = F.elu (self.convd1_1 (conved1_0))\n",
    "        pooled1 = self.pool (conved1_1)\n",
    "        #print (pooled1.size ())\n",
    "\n",
    "        conved2_0 = F.elu (self.convd2_0 (pooled1))\n",
    "        conved2_1 = F.elu (self.convd2_1 (conved2_0))\n",
    "        pooled2 = self.pool (conved2_1)\n",
    "        #print (pooled2.size ())\n",
    "\n",
    "        conved3_0 = F.elu (self.convd3_0 (pooled2))\n",
    "        conved3_1 = F.elu (self.convd3_1 (conved3_0))\n",
    "        dropped1 = self.drop (conved3_1)\n",
    "        pooled3 = self.pool (dropped1)\n",
    "        #print (pooled3.size ())\n",
    "\n",
    "        conved4_0 = F.elu (self.convd4_0 (pooled3))\n",
    "        conved4_1 = F.elu (self.convd4_1 (conved4_0))\n",
    "        dropped2 = self.drop (conved4_1)\n",
    "\n",
    "        #print (\"Minimal size:\", conved4.size ())\n",
    "\n",
    "        # Upsampling\n",
    "        up_sampled1 = self.up_samp1 (dropped2)\n",
    "        up_conved1 = torch.cat ((conved3_1, F.elu (self.up_conv1 (up_sampled1))), dim=1)\n",
    "        conved5_0 = F.elu (self.convu1_0 (up_conved1))\n",
    "        conved5_1 = F.elu (self.convu1_1 (conved5_0))\n",
    "\n",
    "        up_sampled2 = self.up_samp1 (conved5_1)\n",
    "        up_conved2 = torch.cat ((conved2_1, F.elu (self.up_conv2 (up_sampled2))), dim=1)\n",
    "        conved6_0 = F.elu (self.convu2_0 (up_conved2))\n",
    "        conved6_1 = F.elu (self.convu2_1 (conved6_0))\n",
    "\n",
    "        up_sampled3 = self.up_samp1 (conved6_1)\n",
    "        up_conved3 = torch.cat ((conved1_1, F.elu (self.up_conv3 (up_sampled3))), dim=1)\n",
    "        conved7_0 = F.elu (self.convu3_0 (up_conved3))\n",
    "        conved7_1 = F.elu (self.convu3_1 (conved7_0))\n",
    "\n",
    "        # Final horizontal convolution\n",
    "        #conved8 = F.selu (torch.tanh(self.convu3_2(conved7_1)))\n",
    "        #conved9 = F.elu (torch.add (input_data,\n",
    "        #conved8 = torch.add (input_data, torch.tanh(self.convu3_2(conved7_1)))\n",
    "        #conved8 = F.elu (torch.add (input_data, torch.tanh(self.convu3_2(conved7_1))))\n",
    "\n",
    "        #conved8 = F.elu (torch.add (input_data, torch.tanh(self.convu3_2(conved7_1))))\n",
    "        #clamped = torch.clamp (conved8, 0, 1)\n",
    "\n",
    "        conved8 = torch.sigmoid (self.convu3_2 (conved7_1))\n",
    "\n",
    "        #conved8 = torch.sigmoid (torch.add (input_data, torch.tanh(self.convu3_2(conved7_1))))\n",
    "        \n",
    "        # Remover convolutions\n",
    "        #with_input = torch.cat ((input_data, conved8), dim=1)\n",
    "        #print (\"with_input\", with_input.size())\n",
    "        #conved9 = F.selu (self.remover_conv1 (with_input))\n",
    "        #print (\"first\", conved9.size())\n",
    "        #conved10 = torch.sigmoid (self.remover_conv2 (conved9))\n",
    "        #print (\"second\", conved10.size())\n",
    "        \n",
    "        return conved8\n",
    "\n",
    "    def save(self, filename):\n",
    "        args_dict = {\n",
    "            \"in_channels\": self.in_channels,\n",
    "            \"out_channels\": self.out_channels,\n",
    "            \"depth\": self.depth,\n",
    "            \"dropout\": self.dropout\n",
    "        }\n",
    "        torch.save({\n",
    "            \"state_dict\": self.state_dict(),\n",
    "            \"args_dict\": args_dict\n",
    "        }, filename) #model.state_dict(), \"temp_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# Initialising the data generator and model\n",
    "labels_path = \"data/train/nogrid/\"\n",
    "val_path = \"data/val/nogrid/\" \n",
    "conv_depth = [5,7,5]\n",
    "batch_size = 6 # 8 works probably\n",
    "max_imgs = 10000\n",
    "\n",
    "augments = {\"grid_size\" : (30,30),\n",
    "            \"grid_intensity\" : (-1,1),\n",
    "            \"grid_offset_x\" : (0,90),\n",
    "            \"grid_offset_y\" : (0,90),\n",
    "            \"crop\" : (400,400),\n",
    "            \"hflip\" : 0.5,\n",
    "            \"vflip\" : 0.5,\n",
    "            \"angle\" : 0,\n",
    "            \"shear\" : 0,\n",
    "            \"brightness\" : (0.75,1.25),\n",
    "            \"pad\" : (0,0,0,0),\n",
    "            \"contrast\" : (0.5,1)}\n",
    "\n",
    "dataset = Dataset(max_imgs, labels_path, **augments)\n",
    "validation_set = Dataset(max_imgs, val_path, **augments, seed=1337)\n",
    "\n",
    "training_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "validation_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "#model = Degridifier_model(conv_depth).to(processor)\n",
    "model = Unet (3,3).to(processor)\n",
    "loss_function = nn.MSELoss().to(processor)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-2)\n",
    "#print (dataset.__get_all_files__(\"data/nogrid/\"))\n",
    "#dataset.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialising some variables for use in training\n",
    "batches = float(\"inf\")\n",
    "time_diff = 0\n",
    "no_improv = 0\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 50\n",
    "print_stuff = False\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (batch, labels) in enumerate(training_generator):\n",
    "\n",
    "        # Keeping track of stuff\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        #est_time_left = str(time_diff*(min(batches, dataset.__len__()/batch_size) - i) + (time_diff*min(batches, dataset.__len__()/batch_size)) * (epochs - (epoch+1))).split(\".\")[0]\n",
    "        est_time_left = str(time_diff*(min(batches, dataset.__len__()) - i)+time_diff*(epochs-(epoch+1))*min(batches, dataset.__len__()/batch_size)).split(\".\")[0]\n",
    "        \n",
    "        sys.stdout.write(\"\\rEpoch: {0}. Batch: {1}. Min loss: {2:.5f}. Time left: {3}. Best: {4} batches ago.\".format(epoch+1, i+1, min_loss, est_time_left, no_improv))\n",
    "\n",
    "        # Putting data on gpu\n",
    "        batch = batch.to(processor)\n",
    "        labels = labels.to(processor)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "\n",
    "        out = model(batch)\n",
    "        loss = loss_function(out, labels)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        loss\n",
    "        if loss.item() < min_loss:\n",
    "            #model.save(\"temp_best_model.pth\")\n",
    "            #torch.save(model.state_dict(), \"temp_best_model.pth\")\n",
    "            min_loss = loss.item()\n",
    "            no_improv = 0\n",
    "        else:\n",
    "            no_improv += 1\n",
    "        \n",
    "        # For tracking progress\n",
    "        end_time = datetime.now()\n",
    "        time_diff = end_time - start_time\n",
    "    \n",
    "        \n",
    "        if batch.size()[0] == 1:\n",
    "            to_save = torch.cat ((out, labels, batch), dim=2)\n",
    "        else:\n",
    "            to_save = torch.cat ((out.squeeze(0), labels, batch), dim=2)\n",
    "            #print(to_save.size())\n",
    "            save_image(to_save, \"data/artificialgrid/{0}.png\".format(i))\n",
    "        #save_image(batch.squeeze(0), \"/home/cavtheman/skolearbejde/degridifier/data/artificialgrid/{0}.png\".format(i))\n",
    "    val_loss = 0\n",
    "    for batch, labels in validation_generator:\n",
    "        batch = batch.to(processor)\n",
    "        labels = labels.to(processor)\n",
    "        model.eval()\n",
    "        \n",
    "        out = model(batch)\n",
    "        loss = loss_function(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss_list.append(val_loss/len(validation_generator))\n",
    "    \n",
    "model.save(\"temp_model.pth\")\n",
    "print (\"\\nFinished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the loss through the epochs\n",
    "train_placings = np.linspace(0,epoch,len(loss_list))\n",
    "val_placings = np.arange (1, epoch+1)\n",
    "#print(placings)\n",
    "print(val_loss_list)\n",
    "plt.plot(train_placings, loss_list, label=\"Training\")\n",
    "plt.plot(val_placings, val_loss_list, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Loss during training\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "unet_experiment.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
